{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2022, salesforce.com, inc and MILA.  \n",
    "All rights reserved.  \n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root  \n",
    "or https://opensource.org/licenses/BSD-3-Clause  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can I register for the competition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out the [registration form](https://docs.google.com/forms/d/e/1FAIpQLSe2SWnhJaRpjcCa3idq7zIFubRoH0pATLOP7c1Y0kMXOV6U4w/viewform) in order to register for the competition. \n",
    "\n",
    "You will only need to provide an email address and a team name. You will also need to be willing to open-source your code after the competition.\n",
    "\n",
    "After you submit your registration form, we will register it internally. Please allow for upto 1-2 working days for your team name to be registered. You will be notified via email upon successful registration. You will need your team name in order to make submissions towards the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickly train agents with CPU using rllib and create a submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command should install all the pre-requisites automatically.\n",
    "\n",
    "Please make sure that you are using Python 3.7 or older version. Our code does not support 3.8 or newer versions currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/train_with_rllib.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your submission locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you actually upload your submission files, you can also evaluate and score your submission on your end using this script. The evaluation script essentially validates the submission files, performs unit testing and computes the metrics for evaluation. To compute the metrics, we first instantiate a trainer, load the policy model with the saved parameters, and then generate several episode rollouts to measure the impact of the policy on the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/evaluate_submission.py -r Submissions/<submission_number>.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where can I submit my solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: Please register for the competition (see the steps above), if you have not done so. Your team must be registered before you can submit your solutions.*\n",
    "\n",
    "The AI climate competition features 3 tracks.\n",
    "\n",
    "In Track 1, you will propose and implement multilateral agreements to augment the simulator, and train the AI agents in the simulator. We evaluate the learned policies and resulting economic and climate change metrics.\n",
    "\n",
    "- The submission form for Track 1 is [here](https://forms.gle/fuM4NZ5eX2rdckit6).\n",
    "- Or, as an alternative, submit [here](https://workspace.jianguoyun.com/inbox/collect/c7be3a1c61624a4498666095d8a51824/submit) if you have difficulty accessing the Google form\n",
    "\n",
    "\n",
    "Please select your registered team name from the drop-down menu, and upload a zip file containing the submission files - we will be providing scripts to help you create the zip file.\n",
    "\n",
    "In Track 2, you will argue why your solution is practically relevant and usable in the real world. We expect the entries in this track to contain a high-level summary for policymakers\n",
    "\n",
    "To submit the generated running result and your code (the **.zip** file):\n",
    "- The submission form for Track 2 is [here](https://forms.gle/1kTsFLUp6yVF3xQf9).\n",
    "- Or, as an alternative, submit [here](https://workspace.jianguoyun.com/inbox/collect/323b16a8697741348e3197ebccafea81/submit) if you have difficulty accessing the Google form\n",
    "\n",
    "To submit your essay: [OpenReview](https://openreview.net/group?id=AI4ClimateCoop.org/2022/Workshop)\n",
    "\n",
    "In Track 3, we invite you to point out potential simulation loopholes and improvements.\n",
    "\n",
    "To submit your code to support your suggestions (a **.zip** file, optional):\n",
    "- The submission form for Track 3 is [here](https://forms.gle/stBihEdaf58xi2Vr6).\n",
    "- Or, as an alternative, submit [here](https://workspace.jianguoyun.com/inbox/collect/e96127b108ed4172a3b79273688a883c/submit) if you have difficulty accessing the Google form\n",
    "\n",
    "To submit your essay: [OpenReview](https://openreview.net/group?id=AI4ClimateCoop.org/2022/Workshop)\n",
    "\n",
    "If you do not see your team name in the drop-down menu, please contact us on Slack or by e-mail, and we will resolve that for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do I create a submission using my modified negotiation protocol? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the base version of the RICE-N (regional integrated climate environment) simulation environment written in Python (`rice.py`).\n",
    "\n",
    "**For the mathematical background and scientific references, please see [the white paper](https://deliverypdf.ssrn.com/delivery.php?ID=428101121103108016095076093074095111015069058086095042085123117113111092124092117108004117037031126012054120125119115118069067102029022089006118121099082113093096121049050055084110110018003106083072011105122122123113102083083074084083085090104119080101&EXT=pdf&INDEX=TRUE).**\n",
    "\n",
    "You will need to mainly modify the `rice.py` to implement the proposed negotiatoin protocol. Additional details can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts for creating the zipped submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the zipped file required for submission is automatically created post-training. However, for any reason (for example, for providing a trained policy model at a different timestep), you can create the zipped submission yourself using the `create_submizzion_zip.py` script. Accordingly, create a new directory (say `submission_dir`) with all the relevant files (see the section above), and you can then simply invoke\n",
    "```commandline\n",
    "python scripts/create_submission_zip.py -r <PATH-TO-SUBMISSION-DIR>\n",
    "```\n",
    "\n",
    "That will first validate that the submission directory contains all the required files, and then provide you a zipped file that can you use towards your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/create_submission_zip.py -r <PATH-TO-SUBMISSION-DIR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts for unit testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure that all the submissions are consistent in that they comply within the rules of the competition, we have also added unit tests. These are automatically run also when the evaluation is performed. The script currently performs the following tests\n",
    "\n",
    "- Test that the environment attributes (such as the RICE and DICE constants, the simulation period and the number of regions) are consistent with the base environment class that we also provide.\n",
    "- Test that the `climate_and_economy_simulation_step()` is consistent with the base class. As aforementioned, users are free to add different negotiation strategies such as multi-lateral negotiations or climate clubs, but should not modify the equations underlying the climate and economic dynamics in the world.\n",
    "- Test that the environment resetting and stepping yield outputs in the desired format (for instance, observations are a dictionary keyed by region id, and so are rewards.)\n",
    "- If the user used WarpDrive, we also perform consistency checks to verify that the CUDA implementation of the rice environment is consistent with the pythonic version.\n",
    "\n",
    "USAGE: You may invoke the unit tests on a submission file via\n",
    "```commandline\n",
    "python scripts/run_unittests.py -r <PATH-TO-ZIP-FILE>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/run_unittests.py -r <PATH-TO-ZIP-FILE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts for performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USAGE: You may evaluate the submission file using\n",
    "```commandline\n",
    "python scripts/evaluate_submission.py -r <PATH-TO-ZIP-FILE>\n",
    "```\n",
    "Please verify that you can indeed evaluate your submission, before actually uploading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you submit your solution, we will be using the same evaluation script that is provided to you, to score your submissions, but using several rollout episodes to average the metrics such as the average rewards, the global temperature rise, capital, production, and many more. We will then rank the submissions based on the various metrics.The score computed by the evaluation process should be similar to the score computed on your end, since they use the same scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when I make an invalid submission?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An \"invalid submission\" may refer to a submission wherein some or all of the submission files are missing, or the submission files are inconsistent with the base version, basically anything that fails in the evaluation process. Any invalid solution cannot be evaluated, and hence will not feature in the leaderboard. While we can let you know if your submission is invalid, the process is not automated, so we may not be able to do it promptly. To avoid any issues, please use the `create_submission_zip` script to create your zipped submission file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition leaderboard is displayed on the [competition website](https://mila-iqia.github.io/climate-cooperation-competition). After you submit your valid submission, please give it a few minutes to perform an evaluation of your submission and refresh the leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many submissions are allowed per team?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no limit on the number of submissions per team. Feel free to submit as many solutions as you would like. We will only be using your submission with the highest evaluation score towards the leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T13:41:49.356590Z",
     "start_time": "2022-06-13T13:41:22.620490Z"
    },
    "scrolled": true
   },
   "source": [
    "# Code overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the detailed file tree, and file descriptions.\n",
    "```commandline\n",
    "ROOT_DIR\n",
    "├── rice.py\n",
    "├── rice_helpers.py\n",
    "├── region_yamls\n",
    "\n",
    "├── rice_step.cu\n",
    "├── rice_cuda.py\n",
    "├── rice_build.cu\n",
    "\n",
    "└── scripts\n",
    "    ├── train_with_rllib.py\n",
    "    ├── rice_rllib.yaml\n",
    "    ├── torch_models.py\n",
    "    \n",
    "    ├── train_with_warp_drive.py\n",
    "    ├── rice_warpdrive.yaml\n",
    "    ├── run_cpu_gpu_env_consistency_checks.py\n",
    "    \n",
    "    ├── run_unittests.py    \n",
    "    ├── create_submission_zip.py\n",
    "    └── evaluate_submission.py   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `rice.py`: This python script contains the base Rice class. This is written in [OpenAI Gym](https://gym.openai.com/) style with the `reset()` and `step()` functionalities. The step() function comprises an implementation of the `climate_and_economy_simulation_step` which dictate the dynamics of the climate and economy simulation, and should not be altered by the user. We have also provided a simple implementation of bilateral negotiation between regions via the `proposal_step()` and `evaluation_step()` methods. Users can extend the simulation by adding additional proposal strategies, for example, and incorporating them in the `step()` function. However, please do not modify any of the equations dictating the environment dynamics in the `climate_and_economy_simulation_step()`. All the helper functions related to modeling the climate and economic simulation are located in `rice_helpers.py`. Region-specific environment parameters are provided in the `region_yamls` directory.\n",
    "\n",
    "\n",
    "- `rice_step.cu`\n",
    "This is the CUDA C version of the step() function that is required for use with WarpDrive. To get started with WarpDrive, we recommend following these [tutorials](https://github.com/salesforce/warp-drive/tree/master/tutorials). While WarpDrive requires writing the simulation in CUDA C, it also offers orders-of-magnitude speedups for end-to-end training, since it performs rollouts and training all on the GPU. `rice_cuda.py` nd `rice_build.cu` are necessary files for copying simulation data to the GPU and compiling the CUDA code.\n",
    "\n",
    "While implementing the simulation in CUDA C on the GPU offers significantly faster simulations, it requires careful memory management. To make sure that everything works properly, one approach is to first implement your simulation logic in Python. You can then implement the same logic in CUDA C and check the simulation behaviors are the same. To help with this process, we provide an environment consistency checker method to do consistency tests between Python and CUDA C simulations. Before training your CUDA C code, please run the consistency checker to ensure the Python and CUDA C implementations are consistent.\n",
    "```commandline\n",
    "python scripts/run_env_cpu_gpu_consistency_checks.py\n",
    "```\n",
    "\n",
    "See the [tutorial notebook](https://colab.research.google.com/drive/1ifcYaczxy4eHM986fyFeSXarGAKmPLt5#scrollTo=vrIIciaAlHSl) for additional details on modifying the code to implement proposed negotiation protocols."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
