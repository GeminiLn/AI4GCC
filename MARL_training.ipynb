{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2022, salesforce.com, inc and MILA.  \n",
    "All rights reserved.  \n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root  \n",
    "or https://opensource.org/licenses/BSD-3-Clause  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get started quickly with end-to-end multi-agent RL using WarpDrive! This shows a basic example to create a simple Rice environment and perform training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/warp-drive/blob/master/tutorials/simple-end-to-end-example.ipynb)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ PLEASE NOTE:\n",
    "This notebook runs on a GPU runtime.\\\n",
    "If running on Colab, choose Runtime > Change runtime type from the menu, then select `GPU` in the 'hardware accelerator' dropdown menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install the WarpDrive package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install rl-warp-drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from rice_cuda import RiceCuda\n",
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.training.trainer import Trainer\n",
    "from warp_drive.utils.env_registrar import EnvironmentRegistrar\n",
    "\n",
    "pytorch_cuda_init_success = torch.cuda.FloatTensor(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment, Training, and Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = dict(\n",
    "    \n",
    "    # Environment settings\n",
    "    env = dict(  \n",
    "        negotiation_on=1,\n",
    "        num_discretization_cells = 10,\n",
    "    ),\n",
    "\n",
    "    # Trainer settings\n",
    "    trainer = dict(\n",
    "        num_envs = 100,  # Number of environment replicas (numbre of GPU blocks used)\n",
    "        train_batch_size = 10000,  # total batch size used for training per iteration (across all the environments)\n",
    "        num_episodes = 100000,  # Total number of episodes to run the training for (can be arbitrarily high!)\n",
    "    ),\n",
    "    \n",
    "    # Policy network settings\n",
    "    policy =  dict(\n",
    "        regions = dict(\n",
    "            to_train = True,\n",
    "            gamma = 0.92,  # discount factor\n",
    "            lr = 0.0005,  # learning rate\n",
    "            entropy_coeff = [[0,0.5], [1000000, 0.1], [5000000, 0.05]],\n",
    "            vf_loss_coeff = [[0,0.0001], [1000000, 0.001], [5000000, 0.01], [10000000, 0.1]],\n",
    "            model = dict(   \n",
    "                type = \"fully_connected\",\n",
    "                fc_dims = [256,256],  # dimension(s) of the fully connected layers as a list\n",
    "                model_ckpt_filepath = \"\"  # load model parameters from a saved checkpoint (if specified)\n",
    "            )\n",
    "        ),\n",
    "    ),\n",
    "    \n",
    "    # Checkpoint saving setting\n",
    "    saving = dict(\n",
    "        metrics_log_freq = 10,  # How often (in iterations) to print the metrics\n",
    "        model_params_save_freq = 5000,  # How often (in iterations) to save the model parameters\n",
    "        basedir = \"/tmp\",  # base folder used for saving\n",
    "        name = \"rice\",\n",
    "        tag = \"example\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Register the environment\n",
    "env_registrar = EnvironmentRegistrar()\n",
    "this_file_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "env_registrar.add_cuda_env_src_path(\n",
    "    RiceCuda.name,\n",
    "    os.path.join(this_file_dir, \"rice_build.cu\")\n",
    ")\n",
    "\n",
    "# cpu_env = EnvWrapper(Rice())\n",
    "\n",
    "# add_cpu_env = env_registrar.add(device=\"cpu\")\n",
    "# add_cpu_env(cpu_env)\n",
    "# add_gpu_env = env_registrar.add(device=\"gpu\")\n",
    "# add_gpu_env(cpu_env)\n",
    "\n",
    "# Create a wrapped environment object via the EnvWrapper\n",
    "# Ensure that use_cuda is set to True (in order to run on the GPU)\n",
    "env_wrapper = EnvWrapper(\n",
    "    RiceCuda(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"], \n",
    "    use_cuda=True,\n",
    "    env_registrar=env_registrar,\n",
    ")\n",
    "\n",
    "# Agents can share policy models: this dictionary maps policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"regions\": [agent_id for agent_id in range(env_wrapper.env.num_agents)],\n",
    "}\n",
    "\n",
    "# Create the trainer object\n",
    "trainer = Trainer(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    ")\n",
    "\n",
    "# Perform training!\n",
    "trainer.train()\n",
    "\n",
    "# Shut off gracefully\n",
    "# trainer.graceful_close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch episode states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note that any variable registered in rice_cuda.py can be put here\n",
    "desired_outputs = [\n",
    "  \"T_i\", # Temperature\n",
    "  \"M_i\", # Carbon mass\n",
    "  \"sampled_actions\",\n",
    "  \"minMu\"\n",
    "                  ]\n",
    "\n",
    "episode_states = trainer.fetch_episode_states(\n",
    "    desired_outputs\n",
    ")\n",
    "\n",
    "trainer.graceful_close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_T_AT(episode_states, negotiation_on, plot = 0):\n",
    "    state = 'T_i'\n",
    "    if negotiation_on:\n",
    "        values = episode_states[state][::3,0,0]\n",
    "    else:\n",
    "        values =  episode_states[state][:,0,0]\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure() \n",
    "        plt.plot(values[:], label='Temperature - Atmosphere')\n",
    "        fig.legend()\n",
    "        # plt.yscale('log')\n",
    "        fig.show()\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "def get_episode_T_LO(episode_states, negotiation_on, plot = 0):\n",
    "    state = 'T_i'\n",
    "    if negotiation_on:\n",
    "        values = episode_states[state][::3,0,1]\n",
    "    else:\n",
    "        values =  episode_states[state][:,0,1]\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(values[:], label='Temperature - Lower Oceans')\n",
    "        fig.legend()\n",
    "        # plt.yscale('log')\n",
    "        fig.show()\n",
    "\n",
    "        return values\n",
    "\n",
    "def get_episode_M_AT(episode_states, negotiation_on, plot = 0):\n",
    "    state = 'M_i'\n",
    "    if negotiation_on:\n",
    "        values = episode_states[state][::3,0,0]\n",
    "    else:\n",
    "        values =  episode_states[state][:,0,0]\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(values[:], label='Carbon - Atmosphere')\n",
    "        fig.legend()\n",
    "        # plt.yscale('log')\n",
    "        fig.show()\n",
    "\n",
    "    return values\n",
    "\n",
    "def get_episode_M_UP(episode_states, negotiation_on, plot = 0):\n",
    "    state = 'M_i'\n",
    "    if negotiation_on:\n",
    "        values = episode_states[state][::3,0,1]\n",
    "    else:\n",
    "        values =  episode_states[state][:,:0,1]\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(values[:], label='Carbon - Upper Strata')\n",
    "        fig.legend()\n",
    "        # plt.yscale('log')\n",
    "        fig.show()\n",
    "\n",
    "    return values\n",
    "\n",
    "def get_episode_M_UP(episode_states, negotiation_on, plot = 0):\n",
    "    state = 'M_i'\n",
    "    if negotiation_on:\n",
    "        values = episode_states[state][::3,0,2]\n",
    "    else:\n",
    "        values =  episode_states[state][:,0,2]\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(values[:], label='Carbon - Lower Oceans')\n",
    "        fig.legend()\n",
    "        # plt.yscale('log')\n",
    "        fig.show()\n",
    "\n",
    "    return values\n",
    "\n",
    "def get_episode_minMu(episode_states, negotiation_on, plot = 0):\n",
    "    state = 'minMu'\n",
    "    if negotiation_on:\n",
    "        values = episode_states[state][::3,:]\n",
    "    else:\n",
    "        values =  episode_states[state][:,:]\n",
    "\n",
    "    if plot:\n",
    "        for agent in range(len(values[0])):\n",
    "            fig = plt.figure()\n",
    "            plt.plot(values[:,agent], label='minMu -  Agent:' + str(agent))\n",
    "            fig.legend()\n",
    "            # plt.yscale('log')\n",
    "            fig.show()\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_MuAction(episode_states, negotiation_on, plot = 0):\n",
    "    state = 'samples_actions'\n",
    "    if negotiation_on:\n",
    "        values = episode_states[state][::3,:, -2]\n",
    "    else:\n",
    "        values =  episode_states[state][:,:, -2]\n",
    "\n",
    "    if plot:\n",
    "        for agent in range(len(values[0])):\n",
    "            fig = plt.figure()\n",
    "            plt.plot(values[:,agent], label='Mu Action -  Agent:' + str(agent))\n",
    "            fig.legend()\n",
    "            # plt.yscale('log')\n",
    "            fig.show()\n",
    "\n",
    "    return values\n",
    "\n",
    "def get_episode_SavingAction(episode_states, negotiation_on, plot = 0):\n",
    "    state = 'samples_actions'\n",
    "    if negotiation_on:\n",
    "        values = episode_states[state][::3,:, -1]\n",
    "    else:\n",
    "        values =  episode_states[state][:,:, -1]\n",
    "\n",
    "    if plot:\n",
    "        for agent in range(len(values[0])):\n",
    "            fig = plt.figure()\n",
    "            plt.plot(values[:,agent], label='Mu Action -  Agent:' + str(agent))\n",
    "            fig.legend()\n",
    "            # plt.yscale('log')\n",
    "            fig.show()\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_states['sampled_actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_episode_T_AT(episode_states, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_episode_T_LO(episode_states, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_episode_minMu(episode_states, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_episode_M_AT(episode_states, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_AT_reg_0_neg_on = episode_states['T_i'][:,0,0]\n",
    "#T_AT_reg_1_neg_off = episode_states['T_i'][:,1,0]\n",
    "\n",
    "T_LO_reg_0_neg_on = episode_states['T_i'][:,0,1]\n",
    "#T_LO_reg_1_neg_off = episode_states['T_i'][:,1,1]\n",
    "\n",
    "\n",
    "M_AT_reg_0_neg_on = episode_states['M_i'][:,0,0]\n",
    "#M_AT_reg_1_neg_off = episode_states['M_i'][:,1,0]\n",
    "\n",
    "M_UP_reg_0_neg_on = episode_states['M_i'][:,0,1]\n",
    "#M_UP_reg_1_neg_off = episode_states['M_i'][:,1,1]\n",
    "\n",
    "M_LO_reg_0_neg_on = episode_states['M_i'][:,0,2]\n",
    "#M_LO_reg_1_neg_off = episode_states['M_i'][:,1,2]\n",
    "# episode_states_neg_off = episode_states.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_states['minMu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(T_AT_reg_0_neg_on, label='Temperature - Upper Strata - negotiation on')\n",
    "fig.legend()\n",
    "# plt.yscale('log')\n",
    "fig.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(T_LO_reg_0_neg_on, label=\"Temperature - Lower Oceans - negotiation on\")\n",
    "fig.legend()\n",
    "# plt.yscale('log')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(T_AT_reg_0_neg_on, label='Temperature - Upper Strata - negotiation on')\n",
    "fig.legend()\n",
    "# plt.yscale('log')\n",
    "fig.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(T_LO_reg_0_neg_on, label=\"Temperature - Lower Oceans - negotiation on\")\n",
    "fig.legend()\n",
    "# plt.yscale('log')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = dict(\n",
    "    \n",
    "    # Environment settings\n",
    "    env = dict(  \n",
    "        negotiation_on=0,\n",
    "    ),\n",
    "\n",
    "    # Trainer settings\n",
    "    trainer = dict(\n",
    "        num_envs = 100,  # Number of environment replicas (numbre of GPU blocks used)\n",
    "        train_batch_size = 10000,  # total batch size used for training per iteration (across all the environments)\n",
    "        num_episodes = 30000,  # Total number of episodes to run the training for (can be arbitrarily high!)\n",
    "    ),\n",
    "    \n",
    "    # Policy network settings\n",
    "    policy =  dict(\n",
    "        regions = dict(\n",
    "            to_train = True,\n",
    "            gamma = 0.98,  # discount factor\n",
    "            lr = 0.005,  # learning rate\n",
    "            model = dict(   \n",
    "                type = \"fully_connected\",\n",
    "                fc_dims = [256, 256],  # dimension(s) of the fully connected layers as a list\n",
    "                model_ckpt_filepath = \"\"  # load model parameters from a saved checkpoint (if specified)\n",
    "            )\n",
    "        ),\n",
    "    ),\n",
    "    \n",
    "    # Checkpoint saving setting\n",
    "    saving = dict(\n",
    "        metrics_log_freq = 10,  # How often (in iterations) to print the metrics\n",
    "        model_params_save_freq = 5000,  # How often (in iterations) to save the model parameters\n",
    "        basedir = \"/tmp\",  # base folder used for saving\n",
    "        name = \"rice\",\n",
    "        tag = \"example\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environment\n",
    "env_registrar = EnvironmentRegistrar()\n",
    "this_file_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "env_registrar.add_cuda_env_src_path(\n",
    "    RiceCuda.name,\n",
    "    os.path.join(this_file_dir, \"rice_build.cu\")\n",
    ")\n",
    "\n",
    "# cpu_env = EnvWrapper(Rice())\n",
    "\n",
    "# add_cpu_env = env_registrar.add(device=\"cpu\")\n",
    "# add_cpu_env(cpu_env)\n",
    "# add_gpu_env = env_registrar.add(device=\"gpu\")\n",
    "# add_gpu_env(cpu_env)\n",
    "\n",
    "# Create a wrapped environment object via the EnvWrapper\n",
    "# Ensure that use_cuda is set to True (in order to run on the GPU)\n",
    "env_wrapper = EnvWrapper(\n",
    "    RiceCuda(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"], \n",
    "    use_cuda=True,\n",
    "    env_registrar=env_registrar,\n",
    ")\n",
    "\n",
    "# Agents can share policy models: this dictionary maps policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"regions\": [agent_id for agent_id in range(env_wrapper.env.num_agents)],\n",
    "}\n",
    "\n",
    "# Create the trainer object\n",
    "trainer = Trainer(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    ")\n",
    "\n",
    "# Perform training!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note that any variable registered in rice_cuda.py can be put here\n",
    "desired_outputs = [\n",
    "  \"T_i\", # Temperature\n",
    "  \"M_i\", # Carbon mass\n",
    "  \"sampled_actions\",\n",
    "  \"minMu\"\n",
    "                  ]\n",
    "\n",
    "episode_states_neg_off = trainer.fetch_episode_states(\n",
    "    desired_outputs\n",
    ")\n",
    "\n",
    "trainer.graceful_close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_episode_T_AT(episode_states_neg_off, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_episode_M_AT(episode_states_neg_off, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_AT_reg_0_neg_off = episode_states_neg_off['T_i'][:,0,0]\n",
    "#T_AT_reg_1_neg_off = episode_states['T_i'][:,1,0]\n",
    "\n",
    "T_LO_reg_0_neg_off = episode_states_neg_off['T_i'][:,0,1]\n",
    "#T_LO_reg_1_neg_off = episode_states['T_i'][:,1,1]\n",
    "\n",
    "\n",
    "M_AT_reg_0_neg_off = episode_states_neg_off['M_i'][:,0,0]\n",
    "#M_AT_reg_1_neg_off = episode_states['M_i'][:,1,0]\n",
    "\n",
    "M_UP_reg_0_neg_off = episode_states_neg_off['M_i'][:,0,1]\n",
    "#M_UP_reg_1_neg_off = episode_states['M_i'][:,1,1]\n",
    "\n",
    "M_LO_reg_0_neg_off = episode_states_neg_off['M_i'][:,0,2]\n",
    "#M_LO_reg_1_neg_off = episode_states['M_i'][:,1,2]\n",
    "# episode_states_neg_off = episode_states.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(T_AT_reg_0_neg_off, label='Temperature - Upper Strata - negotiation off')\n",
    "fig.legend()\n",
    "# plt.yscale('log')\n",
    "fig.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(T_LO_reg_0_neg_off, label=\"Temperature - Lower Oceans - negotiation off\")\n",
    "fig.legend()\n",
    "# plt.yscale('log')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
